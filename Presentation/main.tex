% --------------------------------------------------- ------------- -------------------------------------------------- %

\documentclass{beamer}

% --------------------------------------------------- Style&&Themes -------------------------------------------------- %

\usetheme{Berkeley}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{navigation symbols}{}

\usecolortheme{rose}
\usepackage[scriptsize]{caption}
\usepackage{pgfplots}

% --------------------------------------------------- Shortcuts?!? --------------------------------------------------- %

\newcommand{\degree}{\ensuremath{^\circ}}
\newcommand\Fontsmall{\fontsize{6}{5}\selectfont}

% --------------------------------------------------- Title Slide! --------------------------------------------------- %

\title{Extrinsic Lidar Calibration for use with Mobile Robotics}

\author{Justin Cosentino}

\institute[Swarthmore College] 
{
  Department of Computer Science\\
  Department of Mathematics\\
  Swarthmore College
  \and
  Perception Systems Group\\
  Intelligent Systems Division\\
  Engineering Laboratory\\
  National Institute of Standards and Technology
}

\date{SURF Colloquium, 2013}

\subject{Lidar Calibration}

\pgfdeclareimage[height=0.5cm]{university-logo}{Images/nist-logo.png}
\logo{\pgfuseimage{university-logo}}

% --------------------------------------------------- ------------- -------------------------------------------------- %

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% --------------------------------------------------- Introduction --------------------------------------------------- %

\section{Introduction}

\subsection{Robotic Perception}
\begin{frame}{Human Perception}
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
        \begin{enumerate}
            \item{Sensory systems:}
            \begin{itemize}
                \item{Visual System}
                \item{Auditory System}
                \item{Somatosensory System}
                \item{Gustatory System}
                \item{Olfactory system}
            \end{itemize}
            \item{Sensory receptors:}
            \begin{itemize}
                \item{Chemosensor}
                \item{Mechanoreceptor}
                \item{Nociceptor}
                \item{Photoreceptor}
                \item{Thermoreceptor}
            \end{itemize}
        \end{enumerate}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{figure}
                \includegraphics[width=.7\textwidth]{Images/eyes.jpg}
                \caption{The human brain maps from one eye to another. \Fontsmall{Image: Copyright Wikipedia.}}
            \end{figure}
        \end{column}
  \end{columns} 
%humans are made up of numerous sensors
%parts of nose, ears, nerve endings, eyes
%all of this is processes by the brain
%see that we are touching something, that is probably what we feel
%in fact, our two eyes must map from one to another
\end{frame}

\begin{frame}{Robotic Perception}
    \begin{columns}[T]
        \begin{column}{.333\textwidth}
            \begin{figure}
                \includegraphics[width=\textwidth]{Images/robot-exterior.png}
                \caption{A PR2 Willow Garage Mobile Research Robot. \Fontsmall{Image: Copyright Willow Garage.}}
            \end{figure}
        \end{column}
        \begin{column}{.666\textwidth}
            \begin{figure}
                \includegraphics[width=.92\textwidth]{Images/stanley.jpg}
                \caption{Stanford's Autonomous Vehicle, Stanley. \Fontsmall{Image: Copyright David Stavens.}}
            \end{figure}
        \end{column}
  \end{columns} 
%just as this is essential for us to navigate, the same can be said for robots
%they too have various sensors - stereo cameras, rangefinders (lidars), etc
%this are necessary for robots to interact with their environment, and without then we cannot perform SLAM, object identification, etc.
%however, how do we know what one lidar is seeing in relation to the other
%this is where lidar calibration comes into play
\end{frame}

\begin{frame}{Robotic Perception}
    \begin{columns}[T]
        \begin{column}{.333\textwidth}
            \begin{figure}
                \includegraphics[width=\textwidth]{Images/robot-interior.png}
                \caption{A Willow Garage PR2 Research Robot. \Fontsmall{Image: Copyright Willow Garage.}}
            \end{figure}
        \end{column}
        \begin{column}{.666\textwidth}
            \begin{figure}
                \includegraphics[width=.86\textwidth]{Images/stanley-scan.png}
                \caption{Point-cloud generated by Stanford's Autonomous Vehicle. \Fontsmall{Image: Copyright David Stavens.}}
            \end{figure}
        \end{column}
  \end{columns} 
%just as this is essential for us to navigate, the same can be said for robots
%they too have various sensors - stereo cameras, rangefinders (lidars), etc
%this are necessary for robots to interact with their environment, and without then we cannot perform SLAM, object identification, etc.
%however, how do we know what one lidar is seeing in relation to the other
%this is where lidar calibration comes into play
\end{frame}


\begin{frame}{What is Sensor Calibration?}
    \begin{columns}[T]  
        \begin{column}{.4\textwidth}
        \begin{block}{Sensor Calibration}
            The process of determining the \alert{intrinsic} and \alert{extrinsic} parameters of a sensor with respect to the world coordinate system.
        \end{block}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{figure}
                \includegraphics[width=.92\textwidth]{Images/camera-laser-calibration.png}
                \caption{Calibration between a stereo camera and lidar. \Fontsmall{Image: Copyright Robert Pless.}}
            \end{figure}
        \end{column}
  \end{columns} 
%calibration itself can be decomposed into two sets of parameters, extrinsic and intrinsic
%intrinsic parameters deal with how the laser samples the environment, and we don't have to worry about it in our research
%extrinsic parameters involve the pose and orientation of a lidar relative to a world coordinate frame
%this is the center of our problem
% in order for a robot to understand the data it is perceiving relative to each other, calibration must exist
\end{frame}

\subsection{Importance of Calibration} 
\begin{frame}{Mapping between Lidars}
    \begin{center}
        \textbf{How do we map data from one lidar to another?}
        
        \begin{figure}
            \centering
            \includegraphics<1>[width=.6\textwidth]{Images/cartesian_0.png}
            \includegraphics<2>[width=.6\textwidth]{Images/cartesian_1.png}
            \includegraphics<3>[width=.6\textwidth]{Images/cartesian_2.png}
            \includegraphics<4>[width=.6\textwidth]{Images/cartesian_3.png}
            \includegraphics<5>[width=.6\textwidth]{Images/cartesian_4.png}
            \caption{Transformation composed of a rotation and translation mapping from coordinate system $\left\{ {A}\right\}$ to coordinate system $\left\{ {B}\right\}$}
        \end{figure}
    \end{center}

    Given two coordinate systems, $\hat{x}$ and $x$, there exists a Euclidean transformation that aligns the two frames such that:
    
    \begin{equation}\label{eq:main}
         \hat{x} = {\Omega}x + \tau 
    \end{equation}

% so we raise the question, given two laser rangefinders, how do we map from one to another
% a rangefinder will allow us to find very precise range measurements
% we know we can find a transformation and rotation between these two sets
\end{frame}

\begin{frame}{The Problem}
In order to calibrate from one sensor to another, we must then solve for the optimal rotation $\Omega$ and translation $\tau$ between the two lidars: 
    
    \begin{equation}
        \hat{x} = {\Omega}x + \tau \tag{\ref{eq:main}}
    \end{equation}
    
    However, we must also account for any noise within the system: 
    
    \begin{equation}
        \hat{x} = {\Omega}x + \tau + \aleph
    \end{equation}
 
    Thus we solve for $\Omega$ and $\tau$ such that (\ref{eq:min}) is minimized:
    
    \begin{equation}\label{eq:min}
        \Sigma^2 = \displaystyle\sum\limits_{i=1}^n {\parallel \hat{x}_i - ({\Omega}x_i + \tau) \parallel^2}
    \end{equation}

% we then must minimize the equation ... 
\end{frame}

\begin{frame}{The Problem}
    \[
        \Sigma^2 = \displaystyle\sum\limits_{i=1}^n {\parallel \hat{x}_i - ({\Omega}x_i + \tau) \parallel^2}
    \]
    We can use two lidars to generate $\hat{x}$ and $x$ \textbf{BUT} 2D data implies 3 DoF (Degrees of Freedom)
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
            \begin{figure}
                \includegraphics[height=.45\textwidth]{Images/points.pdf}
                \caption{Representation of 2 Dimensional Scan Data}
            \end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
                        \begin{figure}
                \includegraphics[height=.45\textwidth]{Images/dof.png}
                \caption{Full Six Degrees of Freedom}
            \end{figure}
        \end{column}
  \end{columns} 
% BUT lidar data is only 2 dimensional
% this means that we can only solve for three degrees of freedom
% however we live in a three dimensional world - we need a solution that gives us 6DoF
\end{frame}

% --------------------------------------------------- The Solution --------------------------------------------------- %

\section{The Proposed Solution}

\subsection{Designing a Target}
\begin{frame}{Designing a Target}
    We create a new calibration target that allows us to:
    \begin{itemize}
        \item {Calculate 3D points form a 2D scan}
        \item {Create a 3D point to point correspondence}
        \item {Solve for an optimal transformation}
    \end{itemize}
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=.75\textwidth]{Images/target.pdf}
                \caption{Prototype Target Design}
            \end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=.3\textwidth]{Images/lasers.jpg}
                \caption{SICK LMS 200 Lidar}
            \end{figure}
        \end{column}
  \end{columns} 
% explain how our new target solves the 2d problem
% we use it to find a bunch of apex points, and thus we have a point to point correspondence we can use our LSF on (show the old equation)
\end{frame}

\subsection{Methodology}
\begin{frame}{Collecting Lidar Scan Data}
    \begin{figure}
        \centering 
        \input{Images/raw_data.tikz}
        \caption{Raw data composed of 30 scans. The lidar is located at the origin.}
    \end{figure}
% show compiled 30 scans
% discuss parameters used for taking scan data
% frame average 30 scans and correct for shot data
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_0.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_1.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_2.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_3.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_4.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_5.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_6.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_7.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Line Fitting Segmented Data}
    \begin{figure}
        \centering 
        \input{Images/avg_data_8.tikz}
        \caption{Average data from 30 scans. The lidar is located at the origin.}
    \end{figure}
\end{frame}

\begin{frame}{Calculating Target Apex}
    \begin{figure}
        \centering 
        \includegraphics[height=.55\textwidth]{Images/circle.png}
        \caption{Determine the position of the apex by calculating distances between each base intersection point ($b_{0}, b_{1}, b_{2}$) and the distance from each base point to the target apex ($s_{0}, s_{1}, s_{2}$)}
    \end{figure}
% describe localization of apex using triangulation
% note that this is only possible because of target construction
\end{frame}

\begin{frame}{Optimal Transformation Between Two Lidars}
    \[
    \Sigma^2 = \displaystyle\sum\limits_{i=1}^n {\parallel \hat{x}_i - ({\Omega}x_i + \tau) \parallel^2} \]
    \begin{figure}
        \centering
        \input{Images/transform_before.tikz}
    \end{figure}
\end{frame}

\begin{frame}{Optimal Transformation Between Two Lidars}
    \[
    \Sigma^2 = \displaystyle\sum\limits_{i=1}^n {\parallel \hat{x}_i - ({\Omega}x_i + \tau) \parallel^2} \]
    \begin{figure}
        \centering
        \input{Images/transform.tikz}
    \end{figure}
\end{frame}

% --------------------------------------------------- Results --------------------------------------------------- %

\section{Verification}

\subsection{Experimental Results}
\begin{frame}{Full 6DoF Transformation between Lidars}
    \begin{figure}
    \vskip-80pt
        \includegraphics<1>[width=1\textwidth]{Images/6dof_rot.pdf}
        \includegraphics<2>[width=1\textwidth]{Images/6dof_trans.pdf}
    \end{figure}
\end{frame}


\begin{frame}{Total Error Across All Transformations}
    \begin{figure}
        \centering
        \includegraphics[width=.9\textwidth]{Images/total_error.jpg}
    \end{figure}
\end{frame}

\begin{frame}{Human Error in Hand Calibration}
    \begin{figure}
        \centering
        \input{Images/hand_calibrate_1.tikz}
    \end{figure}
\end{frame}

\begin{frame}{Human Error in Hand Calibration}
    \begin{figure}
        \centering
        \input{Images/hand_calibrate_2.tikz}
    \end{figure}
\end{frame}

\subsection{Limitations}
\begin{frame}{Sources of Error \& Limitations of Prototype Target}
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
        Limitations:
        \begin{itemize}
            \item{Distance from target}
            \item{Scan height}
            \item{Target rotation}
        \end{itemize}
        Sources of Error:
        \begin{itemize}
            \item{Hand calibration}
            \item{Mixed pixels}
            \item{Target construction}
            \item{Lidar assumptions}
        \end{itemize}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{figure}
                \centering
                \input{Images/limitations.tikz}
            \end{figure}
        \end{column}
    \end{columns} 
\end{frame}

% --------------------------------------------------- Future Work! --------------------------------------------------- %

\section{Future Directions}
\begin{frame}{Future Work}
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
            \begin{itemize}
            \item{Construct a \alert{final target} from more rigid material}
            \item{Integrate a \alert{real-time heuristic} uncertainty propagation}
            \item{Verify method accuracy and precision with \alert{additional lidars}}
        \end{itemize}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{Images/pr2_map.jpg}
            \end{figure}
        \end{column}
    \end{columns} 
\end{frame}


\begin{frame}{Summary}
    \begin{enumerate}
        \item{Calibration is essential within the field of \alert{mobile robotics}}
        \item{We propose a \alert{closed-form} approach to calibration between two lidars using a newly designed target}
        \item{This approach reveals \alert{6DoF pose} using only 2D data}
    \end{enumerate}
    \begin{center}
        \fontsize{50pt}{10pt}\selectfont{ Questions?}
    \end{center}
\end{frame}

\end{document}